import numpy as np
import cv2
import tensorflow as tf
from sklearn.metrics import confusion_matrix, roc_auc_score, average_precision_score, roc_curve, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

import os

def optical_flow(data):
    num_videos, num_frames, height, width, _ = data.shape
    optical_flow_data = np.zeros((num_videos, num_frames - 1, height, width, 2))
    
    for i in range(num_videos):
        for j in range(1, num_frames):
            prev_frame = data[i, j - 1, :, :, 0]
            next_frame = data[i, j, :, :, 0]
            flow = cv2.calcOpticalFlowFarneback(prev_frame, next_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            optical_flow_data[i, j - 1, :, :, :] = flow
    return optical_flow_data

def batch_generator(predictors, labels, batch_size, shuffle=True):
    total_samples = predictors.shape[0]
    
    while True:
        indices = np.arange(total_samples)
        if shuffle:
            np.random.shuffle(indices)
        
        for start_idx in range(0, total_samples, batch_size):
            end_idx = min(start_idx + batch_size, total_samples)
            batch_indices = indices[start_idx:end_idx]
            
            batch_data = tf.convert_to_tensor(predictors[batch_indices].astype('float32'))
            batch_optical_flow = tf.convert_to_tensor(optical_flow(predictors[batch_indices]).astype('float32'))
            batch_labels = tf.convert_to_tensor(labels[batch_indices].astype('float32'))
            
            yield (batch_data, batch_optical_flow), batch_labels

def batch_generator_weighted(predictors, labels, batch_size, class_weights, shuffle=False):
    total_samples = predictors.shape[0]
    
    while True:
        indices = np.arange(total_samples)
        if shuffle:
            np.random.shuffle(indices)
        
        for start_idx in range(0, total_samples, batch_size):
            end_idx = min(start_idx + batch_size, total_samples)
            batch_indices = indices[start_idx:end_idx]
            
            batch_data = tf.convert_to_tensor(predictors[batch_indices].astype('float32'))
            batch_optical_flow = tf.convert_to_tensor(optical_flow(predictors[batch_indices]).astype('float32'))
            batch_labels = tf.convert_to_tensor(labels[batch_indices].astype('float32'))
            
            # Calculate sample weights for the current batch
            batch_sample_weights = np.array([class_weights[np.argmax(label)] for label in batch_labels])
            
            yield (batch_data, batch_optical_flow), batch_labels, batch_sample_weights

# Define the custom callback class
class CustomModelCheckpoint(tf.keras.callbacks.Callback):
    def __init__(self, base_dir, save_weights_only=True, verbose=1):
        super(CustomModelCheckpoint, self).__init__()
        self.base_dir = base_dir  # Directory to save weights
        self.save_weights_only = save_weights_only
        self.verbose = verbose
        self.step_accuracies = []  # Store step accuracies
        self.best_val_accuracy = -np.inf  # Track the best validation accuracy

    def on_train_batch_end(self, batch, logs=None):
        accuracy = logs.get('accuracy')
        if accuracy is not None:
            self.step_accuracies.append(accuracy)

    def on_epoch_end(self, epoch, logs=None):
        val_accuracy = logs.get('val_accuracy')
        accuracy = logs.get('accuracy')
        
        # Save the model at 5th and 10th epoch
        if epoch + 1 == 5 or epoch + 1 == 10:
            checkpoint_path = os.path.join(self.base_dir, f'model_epoch_{epoch + 1}.weights.h5')
            if self.verbose > 0:
                print(f"\nEpoch {epoch + 1}: Saving model to {checkpoint_path}")
            if self.save_weights_only:
                self.model.save_weights(checkpoint_path, overwrite=True)
            else:
                self.model.save(checkpoint_path, overwrite=True)
        
        # Save the model if the validation accuracy is the best so far and close to the training accuracy
        if val_accuracy is not None and accuracy is not None:
            if abs(val_accuracy - accuracy) < 0.05 and val_accuracy > self.best_val_accuracy:
                self.best_val_accuracy = val_accuracy
                checkpoint_path = os.path.join(self.base_dir, 'best_model.weights.h5')
                if self.verbose > 0:
                    print(f"\nEpoch {epoch + 1}: val_accuracy ({val_accuracy}) and accuracy ({accuracy}) difference is less than 0.05 and val_accuracy is the highest so far, saving model to {checkpoint_path}")
                if self.save_weights_only:
                    self.model.save_weights(checkpoint_path, overwrite=True)
                else:
                    self.model.save(checkpoint_path, overwrite=True)


def plot_confusion_matrix(features, labels, model, BATCH_SIZE = 16):
    # Calculate the number of steps for the predictions
    total_samples = len(labels)
    steps = (total_samples // BATCH_SIZE) + 1
    
    eval_gen = batch_generator(features, labels, BATCH_SIZE, shuffle=False)
    loss, accuracy = model.evaluate(eval_gen, steps=steps)
    print("Evaluation Loss:", loss)
    print("Evaluation Accuracy:", accuracy)

    # Generate predictions using the batch generator
    predict_gen = batch_generator(features, labels, BATCH_SIZE, shuffle=False)
    y_pred = model.predict(predict_gen, steps=steps)
    
    # Convert predictions to class labels
    y_pred_argmax = np.argmax(y_pred, axis=1)  # Predicted class indices
    y_test_argmax = np.argmax(labels, axis=1)  # True class indices if one-hot encoded
    
    # Compute the confusion matrix
    conf_matrix = confusion_matrix(y_test_argmax, y_pred_argmax)
    
    # Compute AUC score
    auc_score = roc_auc_score(labels, y_pred, multi_class="ovr")
    
    # Compute Precision-Recall score (Average Precision)
    pr_score = average_precision_score(labels, y_pred, average="macro")
    
    # Plot the confusion matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(f'Confusion Matrix (AUC = {auc_score:.2f}, PR = {pr_score:.2f})')
    plt.show()
    
    # Plot ROC curve for each class
    plt.figure(figsize=(8, 6))
    for i in range(labels.shape[1]):
        fpr, tpr, _ = roc_curve(labels[:, i], y_pred[:, i])
        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_score(labels[:, i], y_pred[:, i]):.2f})')
    
    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for a random classifier
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc='lower right')
    plt.show()
    
    # Plot Precision-Recall curve for each class
    plt.figure(figsize=(8, 6))
    for i in range(labels.shape[1]):
        precision, recall, _ = precision_recall_curve(labels[:, i], y_pred[:, i])
        plt.plot(recall, precision, label=f'Class {i} (AP = {average_precision_score(labels[:, i], y_pred[:, i]):.2f})')
    
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend(loc='lower left')
    plt.show()



